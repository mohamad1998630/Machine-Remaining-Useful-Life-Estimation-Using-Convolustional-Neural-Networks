{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Machine Remaining Useful Life Estimation Using Deep CNN**"
      ],
      "metadata": {
        "id": "zuRYJHu0PM1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting the Machine Remaining Useful Life using Deep Convolutional Neural Network (DCNN)"
      ],
      "metadata": {
        "id": "wtaOsc02Pgky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Imports:\n",
        "\n",
        "Imoprt the necessary files and packages."
      ],
      "metadata": {
        "id": "OzDGTEqbQkRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zKjJI5ylw-p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from PreProcessing import *\n",
        "from model import *\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = torch.device(device)\n"
      ],
      "metadata": {
        "id": "ZzOIZeSUQV14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load N-CMAPSS Dataset"
      ],
      "metadata": {
        "id": "f7BwcchTxLrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#X  will be in the form pandas DataFrame, and the target Y will be a numpy array\n",
        "(X_dev,y_dev),(X_test,y_test) = load_NCMAPSS_data() \n"
      ],
      "metadata": {
        "id": "05OUN8ybxH-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalize the data\n",
        "X_dev_normalized,X_test_normalized = data_normalization(X_dev,X_test)"
      ],
      "metadata": {
        "id": "0ekr5oeoxO9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prepare the Data for training"
      ],
      "metadata": {
        "id": "XccM8Xa-RHgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Implement a custom Dataset  for NCMAPPS Dataset :\n",
        "\n",
        "class NCMAPPS(Dataset):\n",
        "\n",
        "    def __init__(self,x,y,history_len):\n",
        "        # Initialize data\n",
        "        '''\n",
        "        parameters:\n",
        "        x : dataframe that from which we took the sensors measurments only\n",
        "        y : target rul\n",
        "        history_len : the length of the sequennce to be considered for each sample\n",
        "        '''\n",
        "        self.history_len = history_len\n",
        "        c_sensors = x.columns[4:-1] # Extract the sonsors reading\n",
        "        # the coloumns with indecies [0-3] are auxiliary variables\n",
        "\n",
        "        self.x = x[c_sensors]\n",
        "        self.x= torch.tensor(self.x.to_numpy()).float() # Convert to numpy array for easier handling and then for a tensor\n",
        "        self.y= torch.tensor(y)                                                     \n",
        "        self.n_samples = x.shape[0]\n",
        "        \n",
        "    # support indexing such that dataset[i] can be used to get i-th sample\n",
        "    def __getitem__(self, index):\n",
        "        # return the sequence from the index up to the index + history_len \n",
        "        # and the target will be the rul at time == index + history_len\n",
        "\n",
        "        return self.x[index: index+self.history_len, :], self.y[index+self.history_len, :]\n",
        "\n",
        "    # we can call len(dataset) to return the size\n",
        "    def __len__(self):\n",
        "        return self.n_samples-self.history_len\n",
        "\n",
        "\n",
        "\n",
        "def train_validation_split(X_dev,y_dev):\n",
        "  '''\n",
        "  parameters:\n",
        "  X_dev : Development data set(features matrix) which has to be splitted into train and validation sets\n",
        "  y_dev : Development targets to be spliited into train and validation sets\n",
        "\n",
        "  this function took the development set and split it into train and validation\n",
        "  it return the data for two random units as a validation set and the remaining units for the train\n",
        "  '''\n",
        "  units = np.unique(X_dev.unit) # units within the dataset\n",
        "\n",
        "  i,j = np.random.randint(1,np.max(units)+1,2) # generate two random numbers within the range(1,unit with the maximum number)\n",
        "\n",
        "  index = (X_dev.unit==i) + (X_dev.unit == j) # index to selects a two random units as a validation set\n",
        "  \n",
        "  #Train set\n",
        "  X_train = X_dev[~index]\n",
        "  y_train = y_dev[~index]\n",
        "\n",
        "  #Validation\n",
        "  X_validation = X_dev[index]\n",
        "  y_validation = y_dev[index]\n",
        "\n",
        "  return (X_train,y_train),(X_validation,y_validation)"
      ],
      "metadata": {
        "id": "yOeJ70QYxev0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train,y_train),(X_val,y_val) = train_validation_split(X_dev_normalized,y_dev)\n",
        "train_set = NCMAPPS(X_train,y_train,50)\n",
        "val_set = NCMAPPS(X_val,y_val,50)\n",
        "batch_size= 1024\n",
        "# Load whole dataset with DataLoader\n",
        "train_loader = DataLoader(dataset=train_set,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=False)\n",
        "\n",
        "val_loader = DataLoader(dataset=val_set,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=False)"
      ],
      "metadata": {
        "id": "O0RcU3FFxg4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define and Initiate the model"
      ],
      "metadata": {
        "id": "rw0F6m0mRk2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DCNN(nn.Module):\n",
        "    \n",
        "    def __init__(self,num_features,conv1_o,conv1_kh,conv2_o,conv2_kh,fc1_o):\n",
        "\n",
        "      '''\n",
        "      Parameters:\n",
        "      num_features : number of the input features of the data\n",
        "      conv1_o : number of the output channels of the first convolutional layer\n",
        "      conv2_o : // //         //                //    second  //\n",
        "      conv1_kh : the height of the kerenel for the first convolutional layer\n",
        "      conv2_kh : //   //    //                  // seconnd //\n",
        "      fc1_o : the number of the output features of the 1st fully connected layer\n",
        "\n",
        "\n",
        "      '''\n",
        "      super(DCNN, self).__init__()\n",
        "      self.num_features = num_features \n",
        "      self.conv1_o = conv1_o \n",
        "      self.conv1_kh = conv1_kh \n",
        "      self.conv2_o =conv2_o  \n",
        "      self.conv2_kh = conv2_kh\n",
        "        \n",
        "      self.dropout = nn.Dropout(p = 0.2)\\\n",
        "\n",
        "      self.conv1 = nn.Sequential(\n",
        "                                   nn.Conv2d(1, self.conv1_o, \n",
        "                                                kernel_size=(self.conv1_kh, self.num_features)),\n",
        "                                   nn.ReLU(), \n",
        "                                    nn.Conv2d(self.conv1_o, self.conv2_o, \n",
        "                                                kernel_size=(self.conv2_kh, 1)) \n",
        "                                   )\n",
        "      self.max_pooling_layer= nn.MaxPool2d(4,stride=2, padding= 2)\n",
        "        \n",
        "      \n",
        "      self.output_features = self.conv2_o\n",
        "        \n",
        "      self.output = nn.Sequential(\n",
        "                                    nn.Linear(self.output_features, fc1_o),\n",
        "                                    nn.ReLU(), \n",
        "                                    nn.Linear(fc1_o, 1)\n",
        "                                    ) \n",
        "                                    \n",
        "        \n",
        "        \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        X (tensor) [batch_size, time_steps, num_features] # Shaped like image\n",
        "        \"\"\"\n",
        "        batch_size = X.size(0)\n",
        "        \n",
        "        # Convolutional Layer\n",
        "        \n",
        "        out = X.unsqueeze(1) # [batch_size, num_channels=1, time_steps, num_features]\n",
        "        \n",
        "        out = F.relu(self.conv1(out)) # [batch_size, conv2_out_channels, shrinked_time_steps, 1]\n",
        "        \n",
        "        out= self.max_pooling_layer(out)\n",
        "        \n",
        "        out = self.dropout(out)\n",
        "\n",
        "        out = torch.squeeze(out, 3) #\n",
        "        \n",
        "        \n",
        "        out = out.permute(0, 2, 1) # \n",
        "        \n",
        "\n",
        "        out= F.relu(out)\n",
        "        \n",
        "        out = out[:, -1, :] \n",
        "        out = self.dropout(out)\n",
        "        \n",
        "        \n",
        "         \n",
        "        # Output Layer\n",
        "        output = F.relu(self.output(out)) \n",
        "        \n",
        "        \n",
        "        return output"
      ],
      "metadata": {
        "id": "kbrKJJQV1H8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# intiate an object from the model\n",
        "model = DCNN(num_features=27,conv1_o=16,conv2_o=32,conv1_kh=14,conv2_kh=14,fc1_o=16).to(device)\n"
      ],
      "metadata": {
        "id": "_0iem165_mUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "ax3TS5MqpC6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The function that performs the training\n",
        "def training_loop(n_epochs, optimiser, model, loss_fn,batch_size, train_loader,val_loader ):\n",
        "  '''\n",
        "  Parameters:\n",
        "  n_epochs :  number of epochs\n",
        "  optimiser : optimiser used for training\n",
        "  model : the model to be trained \n",
        "  loss_fn : loss function used to train the model\n",
        "  batch_size : the batch size \n",
        "  train_loader : dataloader object to load the training data during the training\n",
        "  val_loader : dataloader object to load the validation data during the training\n",
        "  '''\n",
        "    \n",
        "  train_curve = [] # list to save the mean of the training loss for each epoch\n",
        "  #the elements of this list will be the value of the training loss for each epoch\n",
        "\n",
        "  val_curve = [] # list to save the mean of the validation loss for each epoch \n",
        "  \n",
        "\n",
        "  for epoch in range(0, n_epochs):\n",
        "\n",
        "    train_loss = [] #list to save the mean of the training loss for each iteration\n",
        "    #the elements of this list will be the value of the training loss for iteration\n",
        "  \n",
        "    val_loss = [] # list to save the mean of the training loss for each iteration\n",
        "    \n",
        "    model.train(True)\n",
        "    for i ,(features,targets) in enumerate(train_loader):\n",
        "      features= features.to(device)\n",
        "      targets= targets.to(device)\n",
        "      output_train = model.forward(features) # forwards pass\n",
        "      targets = torch.tensor(targets,dtype=torch.float32)\n",
        "      loss_train = loss_fn(output_train, targets) # calculate the loss\n",
        "      optimiser.zero_grad() # set gradients to zero\n",
        "      loss_train.backward() # backwards pass\n",
        "      optimiser.step() # update model parameters\n",
        "      \n",
        "      train_loss.append(loss_train.item()) #append the value of the training loss of the i-th iteration\n",
        "\n",
        "    model.train(False) # to test the model on the validation set\n",
        "\n",
        "    for i ,(features,targets) in enumerate(val_loader):\n",
        "        features= features.to(device)\n",
        "        targets= targets.to(device)\n",
        "        output_val = model.forward(features) # forwards pass\n",
        "        targets = torch.tensor(targets,dtype=torch.float32)\n",
        "        loss_val = loss_fn(output_val, targets) # calculate loss\n",
        "        val_loss.append(loss_val.item()) #append the value of the validation loss of the i-th iteration\n",
        "\n",
        "        \n",
        "        \n",
        "    print(f\"Epoch {epoch}, Training loss {np.mean(train_loss):.4f},\"\n",
        "                  f\" Validation loss {np.mean(val_loss):.4f}\")\n",
        "    \n",
        "    val_curve.append(np.mean(val_loss)) #append the value of the training loss of the epoch \n",
        "    train_curve.append(np.mean(train_loss)) # #append the value of the validation loss of the epoch\n",
        "    \n",
        "\n",
        "    \n",
        "  return (train_curve,val_curve)"
      ],
      "metadata": {
        "id": "Jl4zJf05pGCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the parameters for training \n",
        "batch_size= 1024\n",
        "n_epochs = 2\n",
        "loss_fn = nn.MSELoss()\n",
        "optim = torch.optim.Adam(model.parameters())\n",
        "\n",
        "#DataLoaders\n",
        "train_loader = DataLoader(dataset=train_set,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=False)\n",
        "\n",
        "val_loader = DataLoader(dataset=val_set,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=False)"
      ],
      "metadata": {
        "id": "ZjhxdupNpPiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "\n",
        "(training_curve,validation_curve) = training_loop(n_epochs=n_epochs,optimiser=optim,model=model,\n",
        "                                      loss_fn=loss_fn,batch_size=batch_size,train_loader=train_loader,val_loader=val_loader)"
      ],
      "metadata": {
        "id": "Wn-6xFvdpPdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction and Evaluation\n"
      ],
      "metadata": {
        "id": "sjySN0U_o4dr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model,data_set):\n",
        "  '''\n",
        "  function to predict the rul \n",
        "  parameter:\n",
        "  model : the trained model to predcit the output\n",
        "  data_set : data_set object to predict the ouput for it\n",
        "  '''\n",
        "  batch_size = 1024\n",
        "  y_pred = np.zeros((len(data_set)+data_set.history_len,1)) # vector to save the predicted output\n",
        "\n",
        "  data_loader = DataLoader(dataset = data_set,batch_size = batch_size,shuffle = False)\n",
        "  for i,(x,y) in enumerate(data_loader):\n",
        "    model.train(False)\n",
        "    x = x.to(device)\n",
        "    y_p = model.forward(x)\n",
        "    y_pred[i*batch_size:(i*batch_size+y_p.shape[0])] = y_p.cpu().detach().numpy()\n",
        "  \n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "TRUbopChdc2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#------------First Evaluate on the Devlopment set (train and validation)-------------#\n",
        "dev_set = NCMAPPS(X_dev_normalized,y_dev,100)\n",
        "y_pred1 = predict(model1,dev_set)\n",
        "\n",
        "mse_train = mean_squared_error(y_dev,y_pred)\n",
        "rmse_train = mean_squared_error(y_dev,y_pred,squared=False)\n",
        "\n",
        "print(f'The mean squared error for train set      :  {np.mean(mse_train):.4f}')\n",
        "print(f'The root mean squared error for train set :  {np.mean(rmse_train):.4f}')"
      ],
      "metadata": {
        "id": "8PPQf6UccliO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the prediction for a specific unit\n",
        "\n",
        "unit = 8 # For the dev set it range from 1 to 9 \n",
        "index = X_dev_normalized.unit==unit\n",
        "\n",
        "\n",
        "plt.plot(y_dev[index],label='True Rul')\n",
        "plt.plot(y_pred1[index],label='Predicted Rul')\n",
        "plt.legend()\n",
        "plt.xlabel('Cycles')\n",
        "plt.ylabel('RUL')\n",
        "plt.title('Predcited Vs True Rul')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Nkzwz249h-ZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------Evalute on Test set -----------------------#\n",
        "test_set = NCMAPPS(X_test_normalized,y_test,100)\n",
        "y_pred = predict(model,test_set)\n",
        "\n",
        "mse_test = mean_squared_error(y_test,y_pred)\n",
        "rmse_test = mean_squared_error(y_test,y_pred,squared=False)\n",
        "\n",
        "print(f'The mean squared error for test set      :  {np.mean(mse_test):.4f}')\n",
        "print(f'The root mean squared error for test set :  {np.mean(rmse_test):.4f}')"
      ],
      "metadata": {
        "id": "0ia5Nxekll97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the prediction for a specific unit\n",
        "\n",
        "unit = 10 # For test set it range from 10 to 15 \n",
        "index = X_test_normalized.unit==unit\n",
        "\n",
        "plt.plot(y_test[index],label='True Rul')\n",
        "plt.plot(y_pred[index],label='Predicted Rul')\n",
        "plt.xlabel('Cycles')\n",
        "plt.ylabel('RUL')\n",
        "plt.title('Predcited Vs True Rul')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l7JCQO4cp2_z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}